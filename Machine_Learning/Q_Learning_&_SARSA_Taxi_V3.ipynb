{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"f2X1v82pVbbG","executionInfo":{"status":"ok","timestamp":1699522482303,"user_tz":-330,"elapsed":685,"user":{"displayName":"MOHAMED HASAN FARIS M","userId":"07393633362063938780"}}},"outputs":[],"source":["import numpy as np\n","import gym"]},{"cell_type":"code","source":["def eps_greedy(Q, s, eps=0.1):\n","    if np.random.uniform(0,1) < eps:\n","      return np.random.randint(Q.shape[1])\n","    else:\n","      return greedy(Q, s)"],"metadata":{"id":"pxw5anUxWQX4","executionInfo":{"status":"ok","timestamp":1699522483120,"user_tz":-330,"elapsed":17,"user":{"displayName":"MOHAMED HASAN FARIS M","userId":"07393633362063938780"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7811fe4f-fade-4f66-8f25-7162ad321e7f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}]},{"cell_type":"markdown","source":["# Greedy Policy\n","> Returining TO Maximum Action State Value"],"metadata":{"id":"M2dbjgt8dL34"}},{"cell_type":"code","source":["def greedy(Q, s):\n","    return np.argmax(Q[s])"],"metadata":{"id":"M0vffW7OXh0k","executionInfo":{"status":"ok","timestamp":1699522483121,"user_tz":-330,"elapsed":14,"user":{"displayName":"MOHAMED HASAN FARIS M","userId":"07393633362063938780"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Policy Testing\n"],"metadata":{"id":"xFSRuP_FcrQY"}},{"cell_type":"code","source":["def run_episodes(env, Q, num_episodes=100, to_print=False):\n","    tot_rew = [] #total reward\n","    state = env.reset()\n","\n","    for _ in range(num_episodes):\n","        done = False\n","        game_rew = 0\n","\n","        while not done:\n","            next_state, rew, done, _ =env.step(greedy(Q, state))\n","\n","            state = next_state\n","            game_rew += rew\n","            if done:\n","                state = env.reset()\n","                tot_rew.append(game_rew)\n","\n","    if to_print:\n","        print('Mean score: %.3f of %1 games!'%(np.mean(tot_rew), num_episodes))\n","\n","    return np.mean(tot_rew)"],"metadata":{"id":"PSfTY4VsXzWO","executionInfo":{"status":"ok","timestamp":1699522483122,"user_tz":-330,"elapsed":15,"user":{"displayName":"MOHAMED HASAN FARIS M","userId":"07393633362063938780"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# **SARSA**\n","* initialize Q Matrix\n","* Decay The Epsilon Until It Reaches The Threshold\n","* Choose Next Action\n","* SARSA Update\n","* Testing The Policy\n","\n","\n","\n","\n","\n"],"metadata":{"id":"Y3OfFwXMfLBP"}},{"cell_type":"code","source":["def SARSA(env, lr=0.01, num_episodes=10000, eps=0.3, gamma=0.95, eps_decay=0.00005):\n","    nA = env.action_space.n\n","    nS = env.observation_space.n\n","\n","    Q = np.zeros((nS, nA))\n","    games_rewards=[]\n","    test_rewards=[]\n","\n","    for ep in range(num_episodes):\n","        state = env.reset()\n","        done = False\n","        tot_rew = 0\n","\n","        if eps>0.01:\n","            eps -= eps_decay\n","\n","\n","        action = eps_greedy(Q, state, eps)\n","\n","        while not done:\n","            next_state, rew, done, _ = env.step(action)\n","\n","            next_action = eps_greedy(Q, next_state, eps)\n","\n","            #Bellman's Equation\n","            Q[state][action] = Q[state][action] + lr*(rew + gamma*Q[next_state][next_action] - Q[state][action])\n","\n","            state = next_state\n","            action = next_action\n","            tot_rew += rew\n","            if done:\n","                games_rewards.append(tot_rew)\n","\n","        if (ep % 300) == 0:\n","              test_rew =run_episodes(env, Q, 1000)\n","              print(\"Episode:{:5d}  Eps:{:2.4f}  Rew:{:2.4f}\".format(ep, eps, test_rew))\n","              test_rewards.append(test_rew)\n","    return Q"],"metadata":{"id":"2Cv5ILXXdEmu","executionInfo":{"status":"ok","timestamp":1699522483122,"user_tz":-330,"elapsed":14,"user":{"displayName":"MOHAMED HASAN FARIS M","userId":"07393633362063938780"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["SARSA - TAXI V3 DATA"],"metadata":{"id":"WB_vs7rtMiMZ"}},{"cell_type":"code","source":["if __name__ == '__main__':\n","    env = gym.make('Taxi-v3')\n","    print(\"SARSA\")\n","    Q_sarsa = SARSA(env, lr=0.1, num_episodes=5000, eps=0.4, gamma=0.95, eps_decay=0.001)"],"metadata":{"id":"L1dBSoXoMyqY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699522520567,"user_tz":-330,"elapsed":37458,"user":{"displayName":"MOHAMED HASAN FARIS M","userId":"07393633362063938780"}},"outputId":"b36fccc1-85c5-438f-89af-684b9b3487d3"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n"]},{"output_type":"stream","name":"stdout","text":["SARSA\n","Episode:    0  Eps:0.3990  Rew:-216.1550\n","Episode:  300  Eps:0.0990  Rew:-199.0160\n","Episode:  600  Eps:0.0100  Rew:-219.7050\n","Episode:  900  Eps:0.0100  Rew:-200.0020\n","Episode: 1200  Eps:0.0100  Rew:-88.0240\n","Episode: 1500  Eps:0.0100  Rew:-85.3510\n","Episode: 1800  Eps:0.0100  Rew:-47.2770\n","Episode: 2100  Eps:0.0100  Rew:-22.2010\n","Episode: 2400  Eps:0.0100  Rew:-3.9250\n","Episode: 2700  Eps:0.0100  Rew:3.9010\n","Episode: 3000  Eps:0.0100  Rew:5.3540\n","Episode: 3300  Eps:0.0100  Rew:5.6510\n","Episode: 3600  Eps:0.0100  Rew:7.7940\n","Episode: 3900  Eps:0.0100  Rew:7.8940\n","Episode: 4200  Eps:0.0100  Rew:7.9680\n","Episode: 4500  Eps:0.0100  Rew:8.1710\n","Episode: 4800  Eps:0.0100  Rew:7.8810\n"]}]},{"cell_type":"markdown","source":["**Q-LEARNING**\n","\n","\n","* initialize Q Matrix\n","*Decay The Epsilon Until It Reaches The Threshold\n","*Choose Next Action\n","*SARSA Update\n","*Testing The Policy\n","\n"],"metadata":{"id":"6GgIaIQuV8zD"}},{"cell_type":"code","source":["def Q_Learning(env, lr=0.01, num_episodes=10000, eps=0.3, gamma=0.95, eps_decay=0.00005):\n","    nA = env.action_space.n\n","    nS = env.observation_space.n\n","\n","    Q = np.zeros((nS, nA))\n","    games_rewards=[]\n","    test_rewards=[]\n","\n","    for ep in range(num_episodes):\n","        state = env.reset()\n","        done = False\n","        tot_rew = 0\n","\n","        if eps>0.01:\n","            eps -= eps_decay\n","\n","        while not done:\n","\n","            action = eps_greedy(Q, state, eps)\n","            next_state, rew, done, _ = env.step(action)\n","\n","            #Bellman's Equation\n","            Q[state][action] = Q[state][action] + lr*(rew + gamma*np.max(Q[next_state]) - Q[state][action])\n","\n","            state = next_state\n","            tot_rew += rew\n","            if done:\n","                games_rewards.append(tot_rew)\n","\n","        if (ep % 300) == 0:\n","              test_rew =run_episodes(env, Q, 1000)\n","              print(\"Episode:{:5d}  Eps:{:2.4f}  Rew:{:2.4f}\".format(ep, eps, test_rew))\n","              test_rewards.append(test_rew)\n","    return Q"],"metadata":{"id":"7I_r9Ko4V6ET","executionInfo":{"status":"ok","timestamp":1699522520568,"user_tz":-330,"elapsed":32,"user":{"displayName":"MOHAMED HASAN FARIS M","userId":"07393633362063938780"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["if __name__ == '__main__':\n","    env = gym.make('Taxi-v3')\n","    print(\"Q_Learning\")\n","    Q_Learning = Q_Learning(env, lr=0.1, num_episodes=5000, eps=0.4, gamma=0.95, eps_decay=0.001)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4iu_Ox42RR9p","executionInfo":{"status":"ok","timestamp":1699522557023,"user_tz":-330,"elapsed":36484,"user":{"displayName":"MOHAMED HASAN FARIS M","userId":"07393633362063938780"}},"outputId":"70fbca6c-b495-438a-a0b6-35dc7ce29474"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Q_Learning\n","Episode:    0  Eps:0.3990  Rew:-303.9770\n","Episode:  300  Eps:0.0990  Rew:-198.9250\n","Episode:  600  Eps:0.0100  Rew:-189.5690\n","Episode:  900  Eps:0.0100  Rew:-151.5960\n","Episode: 1200  Eps:0.0100  Rew:-115.2070\n","Episode: 1500  Eps:0.0100  Rew:-64.1080\n","Episode: 1800  Eps:0.0100  Rew:-40.9940\n","Episode: 2100  Eps:0.0100  Rew:-31.6070\n","Episode: 2400  Eps:0.0100  Rew:-15.2690\n","Episode: 2700  Eps:0.0100  Rew:-9.3810\n","Episode: 3000  Eps:0.0100  Rew:-2.9520\n","Episode: 3300  Eps:0.0100  Rew:-2.4640\n","Episode: 3600  Eps:0.0100  Rew:6.2770\n","Episode: 3900  Eps:0.0100  Rew:7.4100\n","Episode: 4200  Eps:0.0100  Rew:7.8960\n","Episode: 4500  Eps:0.0100  Rew:7.9260\n","Episode: 4800  Eps:0.0100  Rew:8.0020\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"9hcllAjGbYJP","executionInfo":{"status":"ok","timestamp":1699522557024,"user_tz":-330,"elapsed":36,"user":{"displayName":"MOHAMED HASAN FARIS M","userId":"07393633362063938780"}}},"execution_count":8,"outputs":[]}]}